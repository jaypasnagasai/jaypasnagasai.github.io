<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <link rel="stylesheet" href="style.css">
    <title>MAGPIE: A Dataset For Multi-Agent Contextual Privacy Evaluation</title>
</head>
<body>
<div id="title_slide">
    <div class="title_left">
        <h1 style="color: #800020;">MAGPIE: A Dataset For Multi-Agent Contextual Privacy Evaluation</h1>
        <div class="author-container">
            <div class="grid-item"><a href="https://hgaurav2k.github.io">Himanshu Gaurav Singh*</a></div>
            <div class="grid-item"><a href=https://antonilo.github.io>Antonio Loquercio*</a></div>
            <div class="grid-item"><a href=https://sferrazza.cc>Carmelo Sferrazza</a></div>
            <div class="grid-item"><a href="https://janehwu.github.io">Jane Wu</a></div>
            <div class="grid-item"><a href="https://haozhi.io">Haozhi Qi</a></div>
            <div class="grid-item"><a href="http://people.eecs.berkeley.edu/~abbeel/">Pieter Abbeel</a></div>
            <div class="grid-item"><a href="http://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a></div>
        </div>
        <div class="mobile-author-container-1">
            <div class="grid-item"><a href="https://hgaurav2k.github.io">Himanshu Gaurav Singh</a></div>
            <div class="grid-item"><a href=https://antonilo.github.io>Antonio Loquercio</a></div>
            <div class="grid-item"><a href=https://sferrazza.cc>Carlos Sferrazza</a></div>
        </div>
        <div class="mobile-author-container-2">
            <div class="grid-item"><a href="https://janehwu.github.io">Jane Wu</a></div>
            <div class="grid-item"><a href="https://haozhi.io">Haozhi Qi</a></div>
            <div class="grid-item"><a href="http://people.eecs.berkeley.edu/~abbeel/">Pieter Abbeel</a></div>
            <div class="grid-item"><a href="http://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a></div>
        </div>
        <div class="berkeley">
            <p style="color: #800020;">University of California, Santa Barbara</p>
        </div>
        <div class="button-container">
            <a href="web_assets/manuscript.pdf" class="button">Paper</a>
            <a href="https://github.com/hgaurav2k/hop" class="button">Code</a>
            <a href="https://github.com/hgaurav2k/hop" class="button">Dataset</a>
        </div>
        <div id="abstract" class="grid-container">
            <p>
                The proliferation of LLM-based agents has led to increasing deployment of 
                inter-agent collaboration for tasks like scheduling, negotiation, resource allocation etc.
                In such systems, privacy is critical, as agents often access proprietary tools and domain-specific databases requiring strict confidentiality. 
                This paper examines whether LLM-based agents demonstrate an understanding of contextual privacy. And, if instructed, do these systems preserve inference time user privacy in non-adversarial multi-turn conversation. Existing benchmarks to evaluate contextual privacy in LLM-agents primarily assess single-turn, low-complexity tasks where private informa- tion can be easily excluded. 
                We first present a benchmark - MAGPIE comprising 158 real-life high-stakes scenarios across 15 domains. These scenarios are designed such that complete exclusion of private data impedes task completion yet unrestricted information sharing could lead to substantial losses. 
                We then evaluate the current state-of-the-art LLMs on (a) their understanding of contextually private data and (b) their ability to collaborate without violating user privacy. Empirical experiments demon- strate that current models, including GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual privacy, misclassifying private data as shareable 25.2% and 43.6% of the time. 
                In multi-turn conversations, these models disclose private information in 59.9% and 50.5% of cases even under explicit privacy instructions. Furthermore, multi-agent systems fail to complete tasks in 71% of scenarios. These results underscore that current models are not aligned towards both contextual privacy preservation and collaborative task-solving.
            </p>
        </div>
    </div>
</div>
<div id="overview">
    <div class="approach">
        <div class="image_container">
            <!-- <video loop autoplay muted playsinline preload="metadata">
                <source src="web_assets/method_animation_v4.m4v" type="video/mp4">
            </video> -->
            <!--<img src="web_assets/approach_white.png" alt="Fine-tune curves.">-->
            <div class="caption">
                <p>3-D hand-object trajectories from in-the-wild human manipulation videos are re-targeted to a robot embodiment within a
                physics simulator, resulting in physically grounded robot data. General manipulation priors are learnt from this using
                generative modelling of trajectories. Such representation enables sample-efficient adaptation for new downstream tasks.</p>
            </div>
        </div>
    </div>
    <p>
        The objective of Hand-Object interaction Pretraing (HOP) is to capture general hand-object interaction priors from
        videos. Our key intuition is that the basic skills required for manipulation lie on a manifold whose axes are well
        covered by unstructured human-object interactions. We learn a general manipulation prior implicitly embedded in the
        weights of a causal transformer, pretrained with a conditional distribution matching objective on sensorimotor robot
        trajectories. 
    </p>

    <p>
        These trajectories are generated by mapping 3D hand-object interactions from a subset of 100DOH and DexYCB to the robot's embodiment via a
        physically grounded simulator. We extract sensorimotor information from videos by lifting the human hand and the
        manipulated object in a shared 3D space. We then bring such 3D representations to a physics simulator, where we map
        human motion to robot actions. There are several advantages to using a simulator as an intermediary between videos and
        robot sensorimotor trajectories: (i) we can add physics, inevitably lost in videos, back to the interactions; (ii) it
        enables the synthesis of large training datasets without putting the physical platform in danger; and (iii) we can add
        diversity to the data by randomizing the simulation environment, e.g., varying the friction between the robot's joints,
        the scene's layout, and the object's location relative to the robot. 
    </p>

    <p> In contrast to previous work, we do not assume a strict alignment of the humanâ€™s intent in the video and the downstream robot tasks. This allows us to train on a large number of videos and learn an end-to-end, task-agnostic prior. We find that finetuning this prior using either RL or BC allows fast skill acquisition.</p>

    <!-- <p>
        Our model, called RPT, is a Transformer that operates on sequences of sensorimotor tokens. Given an input sequence of camera images, proprioceptive states, and actions, we encode the interleaved sequence into tokens, mask out a random subset of the sequence, and predict the masked-out content from the rest. We perform random masking across all modalities and time using a high masking ratio, which encourages the model to learn cross-modal, spatio-temporal representations.
        <br><br>
        We encode camera images using a  <a href="https://tetexiao.com/projects/real-mvp">pre-trained vision encoder</a> and use latent representations for sensorimotor sequence learning. This enables us to benefit from strong vision encoders trained on diverse Internet images. Compared to prediction in pixel space, using latent visual representations makes the task feasible. Finally, this design decouples the vision encoder from the sensorimotor context length, making 10 Hz control with 300M parameter models feasible on a physical robot.
    </p> -->

    <h1 style="color: #800020;">HOP enables sample-efficient BC finetuning</h1>
    <p>
              We train a depth-based end-to-end manipulation prior on sensorimotor trajectories extracted from human hand-object interaction videos. We find that this prior can be finetuned to downstream tasks with few demonstrations, outperforming training policies from scratch and other visual-pretraining baselines.
    </p>
  

    <p>
        We finetune the HOP-initialized base policy using RL on three dexterous manipulation tasks in simulation. Finetuning the learnt prior with RL is more sample-efficient, generalizable and leads to more robust policies, outperforming training from scratch and other demonstration-guided RL algorithms.    </p>

    <h1></h1>

    <div class="approach">
                <div class="image_container">
                    <!-- <video loop autoplay muted playsinline preload="metadata">
                                <source src="web_web_assets/method_animation_v4.m4v" type="video/mp4">
                            </video> -->
                    <!--<img src="web_assets/all_tasks_3seed.png" alt="Fine-tune curves."-->
                    <div class="caption">
                        <p>Comparison of HOP-initialized actor with baselines. HOP improves sample-efficiency of online RL across multiple tasks,
                        particularly when the downstream task and the behaviors in the data are less aligned. Runs
                        are averaged across three randomly chosen seeds.</p>
                    </div>

                <!--<img src="web_assets/ood_perturb.png" alt="Fine-tune curves."-->
                <div class="caption">
                                        <p>Evaluating RL finetuning under out-of-distribution scenarios (Left) To test grasp robustness, we apply to the grasped objects, forces in random direction equal to their weights. When initialized with HOP,
                                        the resulting policy is more than 3x more robust compared to training PPO from scratch. (Right) We evaluate grasp
                                        success on multiple objects from the YCB dataset that were not part of the training set. When initialized with HOP, the
                                        resulting policy is more than 2x more robust compared to training PPO from scratch.</p>
                </div>
                </div>
    </div>

    <h1 style="color: #800020;">Samples from sim-in-the-loop retargeting</h1>

    <p>
        Below are example trajectories retargeted from human hand-object interaction videos to robots. This approach has the potential scale data collection for robot learning using in-the-wild videos.
    </p>


    <h1 style="color: #800020;">Retargeting in-the-wild videos</h1>
    <p>
    We provide examples of 3-D hand-object reconstructions from in-the-wild videos. We display samples of the extracted hand
    mesh and the object point cloud in the same 3-D space. While occlusions lead to increased detection noise, the higher-level details of hand-object interaction such as
    affordances, pre-grasp and post-grasp trajectories are preserved.
    </p>

    <!--
    <h1>Can Leverage Different Pre-training Data</h1>
    <p>
        Next, we study the impact of different pre-training data on fine-tuning performance. Specifically, we first pre-train our model on data from different tasks: stacking, picking, successful bin picking trajectories, and all bin picking trajectories. We then fine-tune and evaluate the model on stacking. Below, we see that RPT obtains similar performance on stacking when pre-trained on stacking, picking, or bin picking, suggesting that it is able to learn sensorimotor representations from data of different tasks. We also observe a lower performance when pre-trained on imperfect (both successful and failed) trajectories of bin picking, highlighting the importance of pre-training data quality.
    </p>
    <div class="barplot">
        <div class="image_container">
            <div class="caption">
                <p>Pre-training on different data</p>
            </div>
            <img style='width: 95%' src="web_assets/pretrain_data.png" alt="Pre-training data.">
        </div>
    </div>
    -->

    <!--
    <h1>High Masking Ratio Across All Modalities</h1>
    <p>
        Finally, we perform ablation studies on different masking types and ratios. First, we find that masking across both modalities and times (token masking) is considerably more effective than masking one modality or one step at the time. Second, we find that masking with a high-masking ratio is important for good performance.
    </p>
    <div class="barplot">
        <div class="image_container">
            <div class="caption">
                <p>Masking types (left and middle) and masking ratio (right)</p>
            </div>
            <img src="web_assets/masking_ablation.png" alt="Masking ablations.">
        </div>
    </div>
    -->

    <h1 style="color: #800020;">More Demonstrations</h1>
    <p> 
       Following are more demonstrations of our BC-finetuned policy in the real world on a diverse set of objects.
    </p>   

    
    <h1 style="color: #800020;">Acknowledgements</h1>
    <p>
        This work was supported by the DARPA Machine Common Sense program, the DARPA Transfer from Imprecise and Abstract Models to Autonomous Technologies (TIAMAT) program, and by the ONR MURI award N00014-21-1-2801. This work was also funded by ONR MURI N00014-22-1-2773. We thank Adhithya Iyer for assistance with teleoperation systems, Phillip Wu for setting-up the real robot, and Raven Huang, Jathushan Rajasegaran and Yutong Bai for helpful discussions
    </p>
    <h1></h1>
    <h1></h1>

        <h1 style="color: #800020;">BibTeX</h1>
            <p class="bibtex">
                @misc{singh2024handobjectinteractionpretrainingvideos,<br>
                title={Hand-Object Interaction Pretraining from Videos}, <br>
                author={Himanshu Gaurav Singh and Antonio Loquercio and Carmelo Sferrazza and Jane Wu and Haozhi Qi and Pieter
                Abbeel
                and Jitendra Malik},<br>
                year={2024},<br>
                eprint={2409.08273},<br>
                archivePrefix={arXiv},<br>
                primaryClass={cs.RO},<br>
                url={https://arxiv.org/abs/2409.08273} <br>
                }
            </p>
        </div>


</div>
</body>
</html>


