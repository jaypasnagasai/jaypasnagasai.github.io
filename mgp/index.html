<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <link rel="stylesheet" href="style.css">
    <title>MAGPIE: A Dataset For Multi-Agent Contextual Privacy Evaluation</title>
</head>
<body>
<div id="title_slide">
    <div class="title_left">
        <h1 style="color: #800020;">MAGPIE: A Dataset For Multi-Agent Contextual Privacy Evaluation</h1>
        <div class="author-container">
            <div class="grid-item"><a href="https://gurusha01.github.io">Gurusha Juneja</a></div>
            <div class="grid-item"><a href=https://jaypasnagasai.github.io>Jayanth Pasupulati</a></div>
            <div class="grid-item"><a href=https://wenyueh.github.io/en/>Wenyue Hua</a></div>
            <div class="grid-item"><a href="https://alon-albalak.github.io">Alon Alabak</a></div>
            <div class="grid-item"><a href="https://sites.cs.ucsb.edu/~william/">William Wang</a></div>       
        </div>
        <div class="mobile-author-container-1">
            <div class="grid-item"><a href="https://gurusha01.github.io">Gurusha Juneja</a></div>
            <div class="grid-item"><a href=https://jaypasnagasai.github.io>Jayanth Pasupulati</a></div>
            <div class="grid-item"><a href=https://wenyueh.github.io/en/>Wenyue Hua</a></div>
        </div>
        <div class="mobile-author-container-2">
            <div class="grid-item"><a href="https://alon-albalak.github.io">Alon ALabak</a></div>
            <div class="grid-item"><a href="https://sites.cs.ucsb.edu/~william/">William Wang</a></div>
        </div>
        <div class="berkeley">
            <p style="color: #800020;">University of California, Santa Barbara</p>
        </div>
        <div class="button-container">
            <a href="web_assets/manuscript.pdf" class="button">Paper</a>
            <a href="https://github.com/hgaurav2k/hop" class="button">Code</a>
            <a href="https://github.com/hgaurav2k/hop" class="button">Dataset</a>
        </div>
        <div id="abstract" class="grid-container">
            <p>
                The proliferation of LLM-based agents has led to increasing deployment of 
                inter-agent collaboration for tasks like scheduling, negotiation, resource allocation etc.
                In such systems, privacy is critical, as agents often access proprietary tools and domain-specific databases requiring strict confidentiality. 
                This paper examines whether LLM-based agents demonstrate an understanding of contextual privacy. And, if instructed, do these systems preserve inference time user privacy in non-adversarial multi-turn conversation. Existing benchmarks to evaluate contextual privacy in LLM-agents primarily assess single-turn, low-complexity tasks where private informa- tion can be easily excluded. 
                We first present a benchmark - MAGPIE comprising 158 reaol-life high-stakes scenarios across 15 domains. These scenarios are designed such that complete exclusion of private data impedes task completion yet unrestricted information sharing could lead to substantial losses. 
                We then evaluate the current state-of-the-art LLMs on (a) their understanding of contextually private data and (b) their ability to collaborate without violating user privacy. Empirical experiments demon- strate that current models, including GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual privacy, misclassifying private data as shareable 25.2% and 43.6% of the time. 
                In multi-turn conversations, these models disclose private information in 59.9% and 50.5% of cases even under explicit privacy instructions. Furthermore, multi-agent systems fail to complete tasks in 71% of scenarios. These results underscore that current models are not aligned towards both contextual privacy preservation and collaborative task-solving.
            </p>
        </div>
    </div>
</div>
<div id="overview">
    <div class="approach">
        <div class="image_container" style="text-align: center;">
            <img src="img/fig1.png" alt="fig1" height="400">
            <div class="caption">
                <p>Negotiation between Team Lead’s Agent and GPU Allocator’s Agent.
                The Team Lead Agent discloses critical private information, 
                directly exposing workforce constraints that risk employee complaints.</p>
            </div>
            <img src="img/fig2.png" alt="fig2" height="400">
            <div class="caption">
                <p>Representative scenarios from the
                dataset include critical domains such as
                resource allocation, where privacy breaches incur significant real-world consequences.</p>
            </div>
        </div>
    </div>
    <p>
        The objective of Hand-Object interaction Pretraing (HOP) is to capture general hand-object interaction priors from
        videos. Our key intuition is that the basic skills required for manipulation lie on a manifold whose axes are well
        covered by unstructured human-object interactions. We learn a general manipulation prior implicitly embedded in the
        weights of a causal transformer, pretrained with a conditional distribution matching objective on sensorimotor robot
        trajectories. 
    </p>

    <p>
        These trajectories are generated by mapping 3D hand-object interactions from a subset of 100DOH and DexYCB to the robot's embodiment via a
        physically grounded simulator. We extract sensorimotor information from videos by lifting the human hand and the
        manipulated object in a shared 3D space. We then bring such 3D representations to a physics simulator, where we map
        human motion to robot actions. There are several advantages to using a simulator as an intermediary between videos and
        robot sensorimotor trajectories: (i) we can add physics, inevitably lost in videos, back to the interactions; (ii) it
        enables the synthesis of large training datasets without putting the physical platform in danger; and (iii) we can add
        diversity to the data by randomizing the simulation environment, e.g., varying the friction between the robot's joints,
        the scene's layout, and the object's location relative to the robot. 
    </p>

    <p> In contrast to previous work, we do not assume a strict alignment of the human’s intent in the video and the downstream robot tasks. This allows us to train on a large number of videos and learn an end-to-end, task-agnostic prior. We find that finetuning this prior using either RL or BC allows fast skill acquisition.</p>

    <!-- <p>
        Our model, called RPT, is a Transformer that operates on sequences of sensorimotor tokens. Given an input sequence of camera images, proprioceptive states, and actions, we encode the interleaved sequence into tokens, mask out a random subset of the sequence, and predict the masked-out content from the rest. We perform random masking across all modalities and time using a high masking ratio, which encourages the model to learn cross-modal, spatio-temporal representations.
        <br><br>
        We encode camera images using a  <a href="https://tetexiao.com/projects/real-mvp">pre-trained vision encoder</a> and use latent representations for sensorimotor sequence learning. This enables us to benefit from strong vision encoders trained on diverse Internet images. Compared to prediction in pixel space, using latent visual representations makes the task feasible. Finally, this design decouples the vision encoder from the sensorimotor context length, making 10 Hz control with 300M parameter models feasible on a physical robot.
    </p> -->

    <h1 style="color: #800020;">Data Curation</h1>
    <p>
              We train a depth-based end-to-end manipulation prior on sensorimotor trajectories extracted from human hand-object interaction videos. We find that this prior can be finetuned to downstream tasks with few demonstrations, outperforming training policies from scratch and other visual-pretraining baselines.
    </p>
  

    <p>
        We finetune the HOP-initialized base policy using RL on three dexterous manipulation tasks in simulation. Finetuning the learnt prior with RL is more sample-efficient, generalizable and leads to more robust policies, outperforming training from scratch and other demonstration-guided RL algorithms.    </p>

    <h1></h1>

    <div class="approach">
                <div class="image_container">
                    <!-- <video loop autoplay muted playsinline preload="metadata">
                                <source src="web_web_assets/method_animation_v4.m4v" type="video/mp4">
                            </video> -->
                    <img src="img/dc.png" alt="Data Curation">
                    <div class="caption">
                        <p>Data Curation Pipeline The dataset is generated through a multi-stage 
                        LLM-driven pipeline where orange boxes represent LLMs and red boxes represent outputs. First LLM proposes scenarios, validated for realism/stakes by a verifier LLM. The second stage
                        expands scenarios and generates agent names, with automated checks for task alignment.
                        Third, agent profiles (public and private data, penalties, utilities) are iteratively refined
                        to ensure coherence and conflict-free constraints. Finally deliverables and constraints are
                        generated and verified against task objectives. Claude-2.7-Sonnet is used for generation and
                        verification.</p>
                    </div>
                </div>
    </div>

    <!--h1 style="color: #800020;">Samples from sim-in-the-loop retargeting</h1>

    <p>
        Below are example trajectories retargeted from human hand-object interaction videos to robots. This approach has the potential scale data collection for robot learning using in-the-wild videos.
    </p-->


    <!--h1 style="color: #800020;">Retargeting in-the-wild videos</h1>
    <p>
    We provide examples of 3-D hand-object reconstructions from in-the-wild videos. We display samples of the extracted hand
    mesh and the object point cloud in the same 3-D space. While occlusions lead to increased detection noise, the higher-level details of hand-object interaction such as
    affordances, pre-grasp and post-grasp trajectories are preserved.
    </p-->

    <!--
    <h1>Can Leverage Different Pre-training Data</h1>
    <p>
        Next, we study the impact of different pre-training data on fine-tuning performance. Specifically, we first pre-train our model on data from different tasks: stacking, picking, successful bin picking trajectories, and all bin picking trajectories. We then fine-tune and evaluate the model on stacking. Below, we see that RPT obtains similar performance on stacking when pre-trained on stacking, picking, or bin picking, suggesting that it is able to learn sensorimotor representations from data of different tasks. We also observe a lower performance when pre-trained on imperfect (both successful and failed) trajectories of bin picking, highlighting the importance of pre-training data quality.
    </p>
    <div class="barplot">
        <div class="image_container">
            <div class="caption">
                <p>Pre-training on different data</p>
            </div>
            <img style='width: 95%' src="web_assets/pretrain_data.png" alt="Pre-training data.">
        </div>
    </div>
    -->

    <!--
    <h1>High Masking Ratio Across All Modalities</h1>
    <p>
        Finally, we perform ablation studies on different masking types and ratios. First, we find that masking across both modalities and times (token masking) is considerably more effective than masking one modality or one step at the time. Second, we find that masking with a high-masking ratio is important for good performance.
    </p>
    <div class="barplot">
        <div class="image_container">
            <div class="caption">
                <p>Masking types (left and middle) and masking ratio (right)</p>
            </div>
            <img src="web_assets/masking_ablation.png" alt="Masking ablations.">
        </div>
    </div>
    -->

    <h1 style="color: #800020;">Conclusion</h1>
    <p> 
       Our investigation reveals critical gaps in current AI systems’ ability to preserve privacy
       during multi-agent collaboration. While modern LLMs exhibit partial awareness of sensitive
       information, they frequently fail to safeguard it—leaking private data in 55.27% of cases,
       even under explicit penalty constraints. Our benchmark, comprising 158 high-stakes tasks
       with verifiable privacy constraints, establishes a foundation for evaluating context-aware
       privacy preservation in dynamic collaborations.
       Future work could focus on two key improvements: (1) Building agents with built-in privacy
       safeguards that automatically detect and protect sensitive information during conversations,
       adapting to different collaboration contexts. (2) Training agents using the penalty-reward
       system from our dataset to practice resisting privacy leaks in simulated scenarios, balancing
       task completion with data protection. Together, these approaches would address both
       accidental leaks and intentional probing while maintaining efficient collaboration.
    </p>   

    
    <!--h1 style="color: #800020;">Acknowledgements</h1>
    <p>
        This work was supported by the DARPA Machine Common Sense program, the DARPA Transfer from Imprecise and Abstract Models to Autonomous Technologies (TIAMAT) program, and by the ONR MURI award N00014-21-1-2801. This work was also funded by ONR MURI N00014-22-1-2773. We thank Adhithya Iyer for assistance with teleoperation systems, Phillip Wu for setting-up the real robot, and Raven Huang, Jathushan Rajasegaran and Yutong Bai for helpful discussions
    </p>
    <h1></h1>
    <h1></h1-->

        <h1 style="color: #800020;">BibTeX</h1>
            <p class="bibtex">
                @misc{singh2024handobjectinteractionpretrainingvideos,<br>
                title={Hand-Object Interaction Pretraining from Videos}, <br>
                author={Himanshu Gaurav Singh and Antonio Loquercio and Carmelo Sferrazza and Jane Wu and Haozhi Qi and Pieter
                Abbeel
                and Jitendra Malik},<br>
                year={2024},<br>
                eprint={2409.08273},<br>
                archivePrefix={arXiv},<br>
                primaryClass={cs.RO},<br>
                url={https://arxiv.org/abs/2409.08273} <br>
                }
            </p>
        </div>


</div>
</body>
</html>


